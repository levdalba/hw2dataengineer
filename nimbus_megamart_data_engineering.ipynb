{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d8d1f",
   "metadata": {},
   "source": [
    "# Data Engineering at NimbusMegaMart\n",
    "\n",
    "**From:** Sarah Kim, CTO @ NimbusMegaMart  \n",
    "**To:** Data Platform Team  \n",
    "**Subject:** Daily JSON event rollups\n",
    "\n",
    "This notebook implements the requested daily KPI rollups for NimbusMegaMart's JSON event data, including:\n",
    "- Daily aggregations by country Ã— category\n",
    "- Revenue calculations and 7-day rolling windows\n",
    "- Partitioned Parquet output\n",
    "- Repartitioning performance experiments\n",
    "\n",
    "**Requirements:**\n",
    "- Process JSON event data using Spark DataFrame API\n",
    "- Create schemas with StructType/StructField\n",
    "- Implement broadcast joins to minimize shuffles\n",
    "- Generate daily KPIs with rolling revenue calculations\n",
    "- Experiment with repartitioning strategies for performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33593e7",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with appropriate configurations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType, MapType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# Create Spark session with optimized configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NimbusMegaMart-DataEngineering\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Number of cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(\"Spark session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042e4dd",
   "metadata": {},
   "source": [
    "## Section 2: Define Data Schemas\n",
    "\n",
    "Define StructType schemas for all three datasets to ensure proper data type casting and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd26428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for events dataset\n",
    "events_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"ts\", LongType(), True),  # Unix timestamp\n",
    "    StructField(\"props\", MapType(StringType(), StringType()), True)  # Flexible properties map\n",
    "])\n",
    "\n",
    "# Define schema for users dataset\n",
    "users_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for items dataset\n",
    "items_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schemas defined successfully!\")\n",
    "print(f\"Events schema: {len(events_schema.fields)} fields\")\n",
    "print(f\"Users schema: {len(users_schema.fields)} fields\") \n",
    "print(f\"Items schema: {len(items_schema.fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ceea6",
   "metadata": {},
   "source": [
    "## Section 3: Read Datasets with Schema Validation\n",
    "\n",
    "Load the three datasets from JSON files using the predefined schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets with schemas\n",
    "events_df = spark.read.schema(events_schema).json(\"data/events.jsonl\")\n",
    "users_df = spark.read.schema(users_schema).json(\"data/users.jsonl\")\n",
    "items_df = spark.read.schema(items_schema).json(\"data/items.jsonl\")\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(\"Sample records from each dataset:\")\n",
    "print(\"\\n--- Events Dataset ---\")\n",
    "events_df.show(3, truncate=False)\n",
    "print(\"\\n--- Users Dataset ---\")\n",
    "users_df.show(3, truncate=False)\n",
    "print(\"\\n--- Items Dataset ---\")\n",
    "items_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2886c",
   "metadata": {},
   "source": [
    "## Section 4: Data Exploration and Partition Analysis\n",
    "\n",
    "Display record counts and partition numbers for each DataFrame to understand data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6024081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display record counts and partition information\n",
    "print(\"=== DATASET STATISTICS ===\")\n",
    "print(f\"Events: {events_df.count():,} records, {events_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Users: {users_df.count():,} records, {users_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Items: {items_df.count():,} records, {items_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(\"\\n=== EVENTS DATASET DETAILS ===\")\n",
    "print(\"Event type distribution:\")\n",
    "events_df.groupBy(\"event_type\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"Events by day:\")\n",
    "events_df.select(from_unixtime(col(\"ts\"), \"yyyy-MM-dd\").alias(\"date\")) \\\n",
    "    .groupBy(\"date\").count() \\\n",
    "    .orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf377aa",
   "metadata": {},
   "source": [
    "## Section 5: Add Timestamp and Date Columns\n",
    "\n",
    "Transform the 'ts' column to create 'timestamp' and 'date' columns using Spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add timestamp and date columns to events DataFrame\n",
    "events_with_time = events_df.withColumn(\"timestamp\", to_timestamp(from_unixtime(col(\"ts\")))) \\\n",
    "                            .withColumn(\"date\", to_date(from_unixtime(col(\"ts\"))))\n",
    "\n",
    "print(\"Added timestamp and date columns to events DataFrame\")\n",
    "print(\"Sample records with new columns:\")\n",
    "events_with_time.select(\"event_id\", \"ts\", \"timestamp\", \"date\", \"event_type\").show(5, truncate=False)\n",
    "\n",
    "print(\"\\nDate range in the dataset:\")\n",
    "events_with_time.select(min(\"date\").alias(\"start_date\"), max(\"date\").alias(\"end_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c480cc",
   "metadata": {},
   "source": [
    "## Section 6: Calculate Revenue Column\n",
    "\n",
    "Create a 'revenue' column using when().otherwise() logic based on event type and props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create revenue column using when().otherwise() logic\n",
    "events_with_revenue = events_with_time.withColumn(\n",
    "    \"revenue\",\n",
    "    when(col(\"event_type\") == \"purchase\", col(\"props.price\").cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ")\n",
    "\n",
    "print(\"Added revenue column to events DataFrame\")\n",
    "print(\"Sample records with revenue column:\")\n",
    "events_with_revenue.select(\"event_id\", \"event_type\", \"props\", \"revenue\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\nRevenue statistics:\")\n",
    "events_with_revenue.select(\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "    count(when(col(\"revenue\") > 0, True)).alias(\"revenue_events\"),\n",
    "    min(\"revenue\").alias(\"min_revenue\"),\n",
    "    max(\"revenue\").alias(\"max_revenue\")\n",
    ").show()\n",
    "\n",
    "print(\"Revenue distribution by event type:\")\n",
    "events_with_revenue.groupBy(\"event_type\") \\\n",
    "    .agg(sum(\"revenue\").alias(\"total_revenue\"), \n",
    "         count(\"*\").alias(\"event_count\")) \\\n",
    "    .orderBy(desc(\"total_revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752b2ca",
   "metadata": {},
   "source": [
    "## Section 7: Filter Negative Revenue Records\n",
    "\n",
    "Remove rows with negative revenue values to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative revenue records\n",
    "negative_revenue_count = events_with_revenue.filter(col(\"revenue\") < 0).count()\n",
    "total_records_before = events_with_revenue.count()\n",
    "\n",
    "print(f\"Records with negative revenue: {negative_revenue_count:,}\")\n",
    "print(f\"Total records before filtering: {total_records_before:,}\")\n",
    "\n",
    "# Filter out negative revenue records\n",
    "events_clean = events_with_revenue.filter(col(\"revenue\") >= 0)\n",
    "total_records_after = events_clean.count()\n",
    "\n",
    "print(f\"Total records after filtering: {total_records_after:,}\")\n",
    "print(f\"Records removed: {total_records_before - total_records_after:,}\")\n",
    "\n",
    "# Verify no negative revenues remain\n",
    "print(\"\\nRevenue range after filtering:\")\n",
    "events_clean.select(min(\"revenue\").alias(\"min_revenue\"), max(\"revenue\").alias(\"max_revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8dea3",
   "metadata": {},
   "source": [
    "## Section 8: Broadcast Joins for Events, Items, and Users\n",
    "\n",
    "Perform efficient joins using broadcast() function to minimize shuffles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast joins to minimize shuffles\n",
    "# \n",
    "# BROADCAST JOIN EXPLANATION:\n",
    "# Broadcast joins are used when one dataset is much smaller than another.\n",
    "# The smaller dataset is sent to every executor node, eliminating the need\n",
    "# for shuffle operations. This is highly efficient for our use case because:\n",
    "# 1. Items dataset (200 records) << Events dataset (50,000 records)  \n",
    "# 2. Users dataset (1,000 records) << Events dataset (50,000 records)\n",
    "# 3. By broadcasting the smaller datasets, we avoid expensive shuffles that\n",
    "#    would normally be required to co-locate matching records across partitions\n",
    "# 4. This reduces network I/O and improves join performance significantly\n",
    "\n",
    "print(\"=== PERFORMING BROADCAST JOINS ===\")\n",
    "print(\"Broadcasting smaller datasets (items & users) to avoid shuffles...\")\n",
    "\n",
    "# First join: events with items (broadcast items)\n",
    "events_with_items = events_clean.join(\n",
    "    broadcast(items_df),\n",
    "    events_clean.item_id == items_df.item_id,\n",
    "    \"inner\"\n",
    ").drop(items_df.item_id)  # Remove duplicate item_id column\n",
    "\n",
    "print(f\"Events + Items: {events_with_items.count():,} records\")\n",
    "\n",
    "# Second join: events_with_items with users (broadcast users)  \n",
    "events_enriched = events_with_items.join(\n",
    "    broadcast(users_df),\n",
    "    events_with_items.user_id == users_df.user_id,\n",
    "    \"inner\"\n",
    ").drop(users_df.user_id)  # Remove duplicate user_id column\n",
    "\n",
    "print(f\"Final enriched dataset: {events_enriched.count():,} records\")\n",
    "\n",
    "print(\"\\nSample of enriched dataset:\")\n",
    "events_enriched.select(\"event_id\", \"event_type\", \"country\", \"category\", \"revenue\", \"date\") \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f40a88",
   "metadata": {},
   "source": [
    "## Section 9: Daily KPI Aggregation by Country and Category\n",
    "\n",
    "Group data by date, country, and category to calculate key performance indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a044ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily KPIs grouped by date, country, and category\n",
    "daily_kpi = events_enriched.groupBy(\"date\", \"country\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"events_total\"),\n",
    "        count(when(col(\"event_type\") == \"purchase\", True)).alias(\"purchases\"),\n",
    "        sum(\"revenue\").alias(\"revenue\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    ) \\\n",
    "    .orderBy(\"date\", \"country\", \"category\")\n",
    "\n",
    "print(\"=== DAILY KPI AGGREGATION RESULTS ===\")\n",
    "print(f\"Total daily KPI records: {daily_kpi.count():,}\")\n",
    "\n",
    "print(\"\\nSample daily KPI data:\")\n",
    "daily_kpi.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nKPI summary statistics:\")\n",
    "daily_kpi.select(\n",
    "    sum(\"events_total\").alias(\"total_events\"),\n",
    "    sum(\"purchases\").alias(\"total_purchases\"), \n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    sum(\"unique_users\").alias(\"total_unique_users_sum\"),\n",
    "    avg(\"events_total\").alias(\"avg_daily_events\"),\n",
    "    avg(\"revenue\").alias(\"avg_daily_revenue\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nTop performing country-category combinations by revenue:\")\n",
    "daily_kpi.groupBy(\"country\", \"category\") \\\n",
    "    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n",
    "    .orderBy(desc(\"total_revenue\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388382a2",
   "metadata": {},
   "source": [
    "## Section 10: 7-Day Rolling Revenue Window Function\n",
    "\n",
    "Implement window functions to calculate 7-day rolling revenue sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window for 7-day rolling revenue calculation\n",
    "# Partition by country and category, order by date\n",
    "# Frame: current row + 6 preceding rows (7 days total)\n",
    "window_7d = Window.partitionBy(\"country\", \"category\") \\\n",
    "                  .orderBy(\"date\") \\\n",
    "                  .rowsBetween(-6, 0)\n",
    "\n",
    "# Add 7-day rolling revenue column\n",
    "daily_kpi_with_rolling = daily_kpi.withColumn(\n",
    "    \"revenue_7d\", \n",
    "    sum(\"revenue\").over(window_7d)\n",
    ")\n",
    "\n",
    "print(\"=== 7-DAY ROLLING REVENUE CALCULATION ===\")\n",
    "print(\"Added revenue_7d column with 7-day rolling window\")\n",
    "\n",
    "print(\"\\nSample data with 7-day rolling revenue:\")\n",
    "daily_kpi_with_rolling.select(\"date\", \"country\", \"category\", \"revenue\", \"revenue_7d\") \\\n",
    "    .orderBy(\"country\", \"category\", \"date\") \\\n",
    "    .show(15, truncate=False)\n",
    "\n",
    "print(\"\\nExample: 7-day rolling revenue for US Electronics:\")\n",
    "daily_kpi_with_rolling.filter(\n",
    "    (col(\"country\") == \"US\") & (col(\"category\") == \"electronics\")\n",
    ").select(\"date\", \"revenue\", \"revenue_7d\") \\\n",
    ".orderBy(\"date\").show()\n",
    "\n",
    "print(\"Final schema:\")\n",
    "daily_kpi_with_rolling.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c36d62",
   "metadata": {},
   "source": [
    "## Section 11: Write Partitioned Parquet Files\n",
    "\n",
    "Save the daily KPI DataFrame as partitioned Parquet files by date."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
